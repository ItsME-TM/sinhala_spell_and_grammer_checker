{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Thushan\\anaconda3\\envs\\py310_test\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\Thushan\\anaconda3\\envs\\py310_test\\lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch 0: 100%|██████████| 1443/1443 [1:53:36<00:00,  4.72s/it, loss=0.0226] \n",
      "Epoch 1: 100%|██████████| 1443/1443 [1:56:06<00:00,  4.83s/it, loss=0.0041] \n",
      "Epoch 2: 100%|██████████| 1443/1443 [2:07:19<00:00,  5.29s/it, loss=0.00603]  \n",
      "c:\\Users\\Thushan\\anaconda3\\envs\\py310_test\\lib\\site-packages\\transformers\\modeling_utils.py:2618: UserWarning: Moving the following attributes in the config to the generation config: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('TokenizerAdvanced_Bart/bart_sinhala_grammar_checker\\\\tokenizer_config.json',\n",
       " 'TokenizerAdvanced_Bart/bart_sinhala_grammar_checker\\\\special_tokens_map.json',\n",
       " 'TokenizerAdvanced_Bart/bart_sinhala_grammar_checker\\\\vocab.json',\n",
       " 'TokenizerAdvanced_Bart/bart_sinhala_grammar_checker\\\\merges.txt',\n",
       " 'TokenizerAdvanced_Bart/bart_sinhala_grammar_checker\\\\added_tokens.json')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from transformers import AdamW\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the dataset\n",
    "dataset_path = 'Dataset/sinhala_dataset.csv'\n",
    "data = pd.read_csv(dataset_path)\n",
    "\n",
    "# Prepare the data\n",
    "input_texts = \"grammar_error: \" + data['grammar_error_sentence']\n",
    "target_texts = data['corrected_sentence']\n",
    "\n",
    "# Split the data into train and test sets\n",
    "train_inputs, test_inputs, train_targets, test_targets = train_test_split(\n",
    "    input_texts, target_texts, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Load BART tokenizer\n",
    "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-base\")\n",
    "\n",
    "# Define a Dataset class\n",
    "class GrammarCorrectionDataset(Dataset):\n",
    "    def __init__(self, inputs, targets, tokenizer, max_len=128):\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_text = self.inputs.iloc[idx]\n",
    "        target_text = self.targets.iloc[idx]\n",
    "\n",
    "        input_encoding = self.tokenizer(\n",
    "            input_text, truncation=True, padding=\"max_length\", max_length=self.max_len, return_tensors=\"pt\"\n",
    "        )\n",
    "        target_encoding = self.tokenizer(\n",
    "            target_text, truncation=True, padding=\"max_length\", max_length=self.max_len, return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_encoding[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": input_encoding[\"attention_mask\"].squeeze(0),\n",
    "            \"labels\": target_encoding[\"input_ids\"].squeeze(0),\n",
    "        }\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = GrammarCorrectionDataset(train_inputs, train_targets, tokenizer)\n",
    "test_dataset = GrammarCorrectionDataset(test_inputs, test_targets, tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8)\n",
    "\n",
    "# Load BART model\n",
    "model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-base\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Training loop\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    loop = tqdm(train_loader, leave=True)\n",
    "    for batch in loop:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loop.set_description(f\"Epoch {epoch}\")\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained(\"Models/Advanced_Bart/bart_sinhala_grammar_checker\")\n",
    "tokenizer.save_pretrained(\"TokenizerAdvanced_Bart/bart_sinhala_grammar_checker\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corrected Sentence: වාහන ගෙදර ළයි\n"
     ]
    }
   ],
   "source": [
    "# Define a function for inference\n",
    "def correct_sentence(input_sentence):\n",
    "    input_text = \"grammar_error: \" + input_sentence\n",
    "    input_encoding = tokenizer(input_text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
    "    input_encoding = input_encoding.to(device)\n",
    "    outputs = model.generate(input_encoding[\"input_ids\"], max_length=128, num_beams=4, early_stopping=True)\n",
    "    corrected_sentence = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return corrected_sentence\n",
    "\n",
    "# Test the model\n",
    "test_sentence = \"මම ගෙදර යව\"\n",
    "print(\"Corrected Sentence:\", correct_sentence(test_sentence))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
