{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Grammar Error            Original Sentence\n",
      "0                  මම ගමට යනවා.                  මම ගමට යමි.\n",
      "1                ඔහු පාසලට ගිය.             ඔහු පාසලට ගියේය.\n",
      "2            අපි ගඟේ දිය නෑවෙම.          අපි ගඟේ දිය නෑවෙමු.\n",
      "3  ඔවුන් විභාගය ජයග්‍රහණය කළමු.  ඔවුන් විභාගය ජයග්‍රහණය කළහ.\n",
      "4              ඔහු ගෙදරට ආවායය.              ඔහු ගෙදරට ආවේය.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "dataset_path = \"Dataset/dataset2.csv\"  # Path to the dataset\n",
    "data = pd.read_csv(dataset_path)\n",
    "\n",
    "# Display the dataset structure\n",
    "print(data[['Grammar Error', 'Original Sentence']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model - Google/mt5-small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "You are using a model of type mt5 to instantiate a model of type t5. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completed. Loss: 49.62501907348633\n",
      "Epoch 2 completed. Loss: 30.58425521850586\n",
      "Epoch 3 completed. Loss: 47.42890930175781\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('grammar_correction_model\\\\tokenizer_config.json',\n",
       " 'grammar_correction_model\\\\special_tokens_map.json',\n",
       " 'grammar_correction_model\\\\spiece.model',\n",
       " 'grammar_correction_model\\\\added_tokens.json')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "# Load the pre-trained mT5 model\n",
    "model_name = \"google/mt5-small\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Fine-tune the model (example training loop)\n",
    "def fine_tune_model(data, tokenizer, model, epochs=3):\n",
    "    from torch.utils.data import DataLoader\n",
    "    import torch\n",
    "\n",
    "    # Prepare dataset for training\n",
    "    inputs = tokenizer(list(data['Grammar Error']), padding=True, truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "    labels = tokenizer(list(data['Original Sentence']), padding=True, truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "    dataset = torch.utils.data.TensorDataset(inputs.input_ids, labels.input_ids)\n",
    "    dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "    # Define optimizer\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "    # Training loop\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for batch in dataloader:\n",
    "            input_ids, label_ids = batch\n",
    "            outputs = model(input_ids=input_ids, labels=label_ids)\n",
    "            loss = outputs.loss\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(f\"Epoch {epoch + 1} completed. Loss: {loss.item()}\")\n",
    "    return model\n",
    "\n",
    "# Fine-tune the model\n",
    "fine_tuned_model = fine_tune_model(data, tokenizer, model)\n",
    "\n",
    "# Save the fine-tuned model\n",
    "fine_tuned_model.save_pretrained(\"Models/grammar_correction_model\")\n",
    "tokenizer.save_pretrained(\"Tokenizer/grammar_correction_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: මම ගමට යනවා.\n",
      "Corrected: <extra_id_0>voltaоизоляци跬 <extra_id_11>illionramaiyalatanbegrepクロノグラフ~$(ayev\n"
     ]
    }
   ],
   "source": [
    "# Correct a sentence using the fine-tuned model\n",
    "def correct_grammar(sentence, tokenizer, model):\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\", max_length=128, truncation=True)\n",
    "    outputs = model.generate(inputs.input_ids, max_length=128, num_beams=5, early_stopping=True)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Test the grammar checker\n",
    "test_sentence = data['Grammar Error'][0]\n",
    "corrected_sentence = correct_grammar(test_sentence, tokenizer, fine_tuned_model)\n",
    "print(f\"Original: {test_sentence}\")\n",
    "print(f\"Corrected: {corrected_sentence}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer-Based Grammar Checker Accuracy: 0.00%\n"
     ]
    }
   ],
   "source": [
    "# Define the accuracy calculation function\n",
    "def calculate_sentence_accuracy(original_sentences, corrected_sentences):\n",
    "\tcorrect_count = 0\n",
    "\tfor original, corrected in zip(original_sentences, corrected_sentences):\n",
    "\t\tif original == corrected:\n",
    "\t\t\tcorrect_count += 1\n",
    "\treturn (correct_count / len(original_sentences)) * 100\n",
    "\n",
    "# Evaluate accuracy for the fine-tuned model\n",
    "corrected_sentences = [correct_grammar(sentence, tokenizer, fine_tuned_model) for sentence in data['Grammar Error']]\n",
    "transformer_accuracy = calculate_sentence_accuracy(data['Original Sentence'], corrected_sentences)\n",
    "print(f\"Transformer-Based Grammar Checker Accuracy: {transformer_accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model - Facebook/bart-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Loss: 5.7914\n",
      "Epoch 2/3, Loss: 4.5640\n",
      "Epoch 3/3, Loss: 2.9543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Thushan\\anaconda3\\envs\\py310_test\\lib\\site-packages\\transformers\\modeling_utils.py:2618: UserWarning: Moving the following attributes in the config to the generation config: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('Tokenizer/bart_grammar_checker\\\\tokenizer_config.json',\n",
       " 'Tokenizer/bart_grammar_checker\\\\special_tokens_map.json',\n",
       " 'Tokenizer/bart_grammar_checker\\\\vocab.json',\n",
       " 'Tokenizer/bart_grammar_checker\\\\merges.txt',\n",
       " 'Tokenizer/bart_grammar_checker\\\\added_tokens.json')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "dataset_path = \"Dataset/dataset2.csv\"  # Update with your dataset path\n",
    "data = pd.read_csv(dataset_path)\n",
    "\n",
    "# Define Dataset class\n",
    "class GrammarDataset(Dataset):\n",
    "    def __init__(self, input_texts, target_texts, tokenizer, max_length=128):\n",
    "        self.input_texts = input_texts\n",
    "        self.target_texts = target_texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_text = self.input_texts[idx]\n",
    "        target_text = self.target_texts[idx]\n",
    "\n",
    "        inputs = self.tokenizer(\n",
    "            input_text, max_length=self.max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n",
    "        )\n",
    "        targets = self.tokenizer(\n",
    "            target_text, max_length=self.max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": inputs[\"input_ids\"].squeeze(),\n",
    "            \"attention_mask\": inputs[\"attention_mask\"].squeeze(),\n",
    "            \"labels\": targets[\"input_ids\"].squeeze(),\n",
    "        }\n",
    "\n",
    "# Initialize the tokenizer and model\n",
    "tokenizer.src_lang = \"si_LK\"\n",
    "model_name = \"facebook/bart-base\"  # Use \"bart-large\" for better performance\n",
    "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Prepare data for training\n",
    "input_texts = list(data[\"Grammar Error\"])\n",
    "target_texts = list(data[\"Original Sentence\"])\n",
    "dataset = GrammarDataset(input_texts, target_texts, tokenizer)\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Training loop\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for batch in dataloader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {epoch_loss / len(dataloader):.4f}\")\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model.save_pretrained(\"Models/bart_grammar_checker\")\n",
    "tokenizer.save_pretrained(\"Tokenizer/bart_grammar_checker\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence: අපි ගමට යනවා.\n",
      "Corrected Sentence: ගමට යනවා.\n"
     ]
    }
   ],
   "source": [
    "# Load the fine-tuned BART model\n",
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "\n",
    "model = BartForConditionalGeneration.from_pretrained(\"Models/bart_grammar_checker\")\n",
    "tokenizer = BartTokenizer.from_pretrained(\"Tokenizer/bart_grammar_checker\")\n",
    "\n",
    "# Grammar correction function\n",
    "def correct_grammar(sentence):\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\", max_length=128, truncation=True)\n",
    "    outputs = model.generate(inputs[\"input_ids\"].to(device), max_length=128, num_beams=5, early_stopping=True)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Test the model\n",
    "test_sentence = \"අපි ගමට යනවා.\"\n",
    "corrected_sentence = correct_grammar(test_sentence)\n",
    "print(\"Original Sentence:\", test_sentence)\n",
    "print(\"Corrected Sentence:\", corrected_sentence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test print the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1:\n",
      "  Spelling Mistakes and Corrections: [('මම', 'ම\\u0df8'), ('ගමට', 'ගයනවා.')]\n",
      "  Corrected Sentence: ම෸ ගයනවා.\n",
      "\n",
      "Sentence 2:\n",
      "  Spelling Mistakes and Corrections: [('ගිය.', 'ොිය.')]\n",
      "  Corrected Sentence: ඔහු පාසලට ොිය.\n",
      "\n",
      "Sentence 3:\n",
      "  Spelling Mistakes and Corrections: [('ගඟේ', 'ගඟේය'), ('දිය', '\\u0df1ෑවෙම.')]\n",
      "  Corrected Sentence: අපි ගඟේය ෱ෑවෙම.\n",
      "\n",
      "Sentence 4:\n",
      "  Spelling Mistakes and Corrections: [('විභාගය', 'සිභාගය'), ('ජයග්\\u200dරහණය', 'ජක\\u0d98\\u0dc9\\u200dරහණම\\u0dce.')]\n",
      "  Corrected Sentence: ඔවුන් සිභාගය ජක඘෉‍රහණම෎.\n",
      "\n",
      "Sentence 5:\n",
      "  Spelling Mistakes and Corrections: [('ආවායය.', 'ෆවාය\\u0dfa.')]\n",
      "  Corrected Sentence: ඔහු ගෙදරට ෆවාය෺.\n",
      "\n",
      "Sentence 6:\n",
      "  Spelling Mistakes and Corrections: [('ගුරුවරයා', 'ගුර්ව\\u0d98යා'), ('පාඩම', 'පිඩම'), ('ඉගැන්වීමුමු.', '\\u0dc9ජැන\\u0dccසීභ����\\u0df7�.')]\n",
      "  Corrected Sentence: ගුර්ව඘යා පිඩම ෉ජැන෌සීභ����෷�.\n",
      "\n",
      "Sentence 7:\n",
      "  Spelling Mistakes and Corrections: [('වර්ධනය', 'ව\\u0dfb්ධ\\u0df1\\u0dfa'), ('වේ.', 'Â�����.')]\n",
      "  Corrected Sentence: මේ නගරය ව෻්ධ෱෺ Â�����.\n",
      "\n",
      "Sentence 8:\n",
      "  Spelling Mistakes and Corrections: [('මගේ', 'මිතුරා'), ('මිතුරා', '\\u0df8ට'), ('මට', 'උදව්කළේය.')]\n",
      "  Corrected Sentence: මිතුරා ෸ට උදව්කළේය.\n",
      "\n",
      "Sentence 9:\n",
      "  Spelling Mistakes and Corrections: [('අපේ', 'අපේසල'), ('පාසල', 'ග්\\u200dරථම'), ('ප්\\u200dරථම', 'ි\\u0dccදානයටකජ'), ('ස්ථානයට', '൴\\u0d80ැ\\u0d98ුණවභ�.')]\n",
      "  Corrected Sentence: අපේසල ග්‍රථම ි෌දානයටකජ ൴඀ැ඘ුණවභ�.\n",
      "\n",
      "Sentence 10:\n",
      "  Spelling Mistakes and Corrections: [('ව්\\u200dයාපෘතිය', 'ස\\u0dcdයාපෘතික'), ('සාර්ථකව', 'ඃ�ේමග'), ('නිම', '\\u0ba7හථජ\\u0d98ළභ�.ද\\u0de2.')]\n",
      "  Corrected Sentence: ඔවුන් ස෍යාපෘතික ඃ�ේමග ஧හථජ඘ළභ�.ද෢.\n",
      "\n",
      "Sentence 11:\n",
      "  Spelling Mistakes and Corrections: [('හොඳ.', 'හෘඳ.')]\n",
      "  Corrected Sentence: මේ පොත හෘඳ.\n",
      "\n",
      "Sentence 12:\n",
      "  Spelling Mistakes and Corrections: [('මම', 'ම\\u0df8'), ('යනව.', '\\u0dfaනව.')]\n",
      "  Corrected Sentence: ම෸ අද පාසලට ෺නව.\n",
      "\n",
      "Sentence 13:\n",
      "  Spelling Mistakes and Corrections: [('ගමන', 'ගම\\u0df1'), ('අපි', '\\u0bc3පි'), ('සතුටු', 'ශතුටී'), ('වියෙම.', '\\u0d80ඒකෙභ�.')]\n",
      "  Corrected Sentence: ගම෱ අවසානයේ ௃පි ශතුටී ඀ඒකෙභ�.\n",
      "\n",
      "Sentence 14:\n",
      "  Spelling Mistakes and Corrections: [('මිතුරාගේ', 'මිතුෘරාජක'), ('නම', 'Â��ය'), ('කුමද?', '\\u0de2්\\u0d98෯ද?')]\n",
      "  Corrected Sentence: ඔබගේ මිතුෘරාජක Â��ය ෢්඘෯ද?\n",
      "\n",
      "Sentence 15:\n",
      "  Spelling Mistakes and Corrections: [('රැකියාව', 'රැකියාභ�'), ('ආරම්භ', 'ෆ\\u0dfbම\\u0dccබ�'), ('කළහ.', 'අ�අහ.')]\n",
      "  Corrected Sentence: ඔවුන් රැකියාභ� ෆ෻ම෌බ� අ�අහ.\n",
      "\n",
      "Sentence 16:\n",
      "  Spelling Mistakes and Corrections: [('මම', 'ම\\u0df8'), ('පිළිබඳව', '෴ිළසබඳ්තර'), ('විස්තර', 'අ�හෙභ.')]\n",
      "  Corrected Sentence: ම෸ ගැටලුව ෴ිළසබඳ්තර අ�හෙභ.\n",
      "\n",
      "Sentence 17:\n",
      "  Spelling Mistakes and Corrections: [('විය.', '\\u0d80ිය.')]\n",
      "  Corrected Sentence: ගමන සාර්ථක ඀ිය.\n",
      "\n",
      "Sentence 18:\n",
      "  Spelling Mistakes and Corrections: [('ව්\\u200dයාපෘතියක්', 'ස්\\u200dයාඵෘතුක\\u0dcd'), ('ලැබුනා.', 'අ�ැබේභ�.')]\n",
      "  Corrected Sentence: අපිට නව ස්‍යාඵෘතුක෍ අ�ැබේභ�.\n",
      "\n",
      "Sentence 19:\n",
      "  Spelling Mistakes and Corrections: [('මම', 'මියගෘහස්ත'), ('ගෘහස්ත', 'කාරුථ\\u0d84'), ('කාර්යය', '\\u0dc9\\u0d98දනවභ�'), ('ඉක්මනින්', 'අ�ඒේඔළධ\\u0dcd.')]\n",
      "  Corrected Sentence: මියගෘහස්ත කාරුථ඄ ෉඘දනවභ� අ�ඒේඔළධ෍.\n",
      "\n",
      "Sentence 20:\n",
      "  Spelling Mistakes and Corrections: [('පරීක්ෂණය', 'ප\\u0dfbීක්ෂණය'), ('සාර්ථක', 'සාරඊථිලව.')]\n",
      "  Corrected Sentence: ඔහුගේ ප෻ීක්ෂණය සාරඊථිලව.\n",
      "\n",
      "Sentence 21:\n",
      "  Spelling Mistakes and Corrections: [('මගේ', '\\u0df8ගේ'), ('ලේඛනයේ', '\\u0dfd\\u0dfaඛභය්'), ('සටහන්', '\\u0bc3ටහදු'), ('වේ.', 'ీි.')]\n",
      "  Corrected Sentence: ෸ගේ නම ෽෺ඛභය් ௃ටහදු ీි.\n",
      "\n",
      "Sentence 22:\n",
      "  Spelling Mistakes and Corrections: [('ගිගුම්', 'ො\\u0de2\\u0d98ුක්සහෘතවැළ\\u0dccය\\u0dceථෞ.')]\n",
      "  Corrected Sentence: අපි ගමනේදී ො෢඘ුක්සහෘතවැළ෌ය෎ථෞ.\n",
      "\n",
      "Sentence 23:\n",
      "  Spelling Mistakes and Corrections: [('විශේෂණය', 'සිශකෂණයහොඳීභ\\u0dcd'), ('හොඳින්', 'ඃදළ\\u0d98\\u0dc9'), ('සදහන්', 'ເජ.')]\n",
      "  Corrected Sentence: ඔවුන්ගේ සිශකෂණයහොඳීභ෍ ඃදළ඘෉ ເජ.\n",
      "\n",
      "Sentence 24:\n",
      "  Spelling Mistakes and Corrections: [('මේ', 'මේගඟ'), ('ගඟ', 'දිය'), ('දිය', 'ො\\u0dfdකු')]\n",
      "  Corrected Sentence: මේගඟ දිය ො෽කු\n",
      "\n",
      "Sentence 25:\n",
      "  Spelling Mistakes and Corrections: [('ඔවුන්', 'ඔවුන්මිභ�'), ('විසින්', 'ගෘහස\\u0dccත'), ('ගෘහස්ත', 'ේාර\\u0dcdයක'), ('කාර්යය', 'අ�ජ'), ('සාර්ථකව', '\\u0bc3\\u0c4fළථශ.')]\n",
      "  Corrected Sentence: ඔවුන්මිභ� ගෘහස෌ත ේාර෍යක අ�ජ ௃౏ළථශ.\n",
      "\n",
      "Sentence 26:\n",
      "  Spelling Mistakes and Corrections: [('පවුලේ', 'සවුලි'), ('සතුන්', 'ృත්'), ('ඉතා', 'ඉථාන\\u0dcd'), ('සන්තෝසයෙන්', 'අ'), ('සිටිත.', 'suscept')]\n",
      "  Corrected Sentence: අපේ සවුලි ృත් ඉථාන෍ අ suscept suscept susceptibles suscept susceptiles suscept susceptives suscept susceptures suscept susceptimes suscept susceptikes suscept susceptites suscept susceptashes suscept suscepties suscept susceptanches suscept susceptices suscept susceptacks suscept susceptapes suscept susceptales suscept susceptates suscept susceptide suscept susceptumps suscept susceptumbles susceptilesitesitesites susceptimesites susceptitesiteside susceptilesiles susceptiles\n",
      "\n",
      "Sentence 27:\n",
      "  Spelling Mistakes and Corrections: [('නිවැරදි', '\\u0df1ිවැනදී'), ('වේ.', 'අ�ජ.')]\n",
      "  Corrected Sentence: ඔහුගේ ප්‍රකාශය ෱ිවැනදී අ�ජ.\n",
      "\n",
      "Sentence 28:\n",
      "  Spelling Mistakes and Corrections: [('අලුත්', '�ලුත්'), ('මාර්ගය', 'මාරහගය'), ('පරීක්ෂා', '൴නීක\\u0dcdෂව'), ('කළෙම.', 'අ�අෙභ�.')]\n",
      "  Corrected Sentence: අපි �ලුත් මාරහගය ൴නීක෍ෂව අ�අෙභ�.\n",
      "\n",
      "Sentence 29:\n",
      "  Spelling Mistakes and Corrections: [('ඔබගේ', 'ඔබගේනම'), ('නම', 'ලෘඛභයි'), ('ලේඛනයේ', '෯ැක්\\u0d98ා?')]\n",
      "  Corrected Sentence: ඔබගේනම ලෘඛභයි ෯ැක්඘ා?\n",
      "\n",
      "Sentence 30:\n",
      "  Spelling Mistakes and Corrections: [('ජීවිත', 'ජීහිත'), ('සාර්ථකයි.', 'සාරෘථකය\\u0de2.')]\n",
      "  Corrected Sentence: ඔවුන්ගේ ජීහිත සාරෘථකය෢.\n",
      "\n",
      "Sentence 31:\n",
      "  Spelling Mistakes and Corrections: [('මම', 'ම\\u0df8'), ('කාර්යය', 'ේාරුයක'), ('සාර්ථකව', 'අ�ඏ\\u0dfbථිනව\\u0d98ජළෙ\\u0df7�.')]\n",
      "  Corrected Sentence: ම෸ ගෘහස්ත ේාරුයක අ�ඏ෻ථිනව඘ජළෙ෷�.\n",
      "\n",
      "Sentence 32:\n",
      "  Spelling Mistakes and Corrections: [('අවසානයේ', 'ිකවො\\u0dccනයේ'), ('නාට්\\u200dයයක්', 'අ�\\u0dcdටෘ\\u200d\\u0dfaථු\\u0d98හභ�ජඔළණී.')]\n",
      "  Corrected Sentence: අද පාසල් ිකවො෌නයේ අ�෍ටෘ‍෺ථු඘හභ�ජඔළණී.\n",
      "\n",
      "Sentence 33:\n",
      "  Spelling Mistakes and Corrections: [('ඔහුගෙන්', 'ඔහුගෙ\\u0df1්'), ('අවශ්\\u200dයතාවය', 'අසශ\\u0dcc\\u200dයතාේක'), ('ලබා', 'ලබ\\u0dcd'), ('ගත', '൜භ�.')]\n",
      "  Corrected Sentence: ඔහුගෙ෱් වටින අසශ෌‍යතාේක ලබ෍ ൜භ�.\n",
      "\n",
      "Sentence 34:\n",
      "  Spelling Mistakes and Corrections: [('ප්\\u200dරධාන', '෴ඊ\\u200dදධාභ'), ('මාර්ගය', 'අ�වජ්අ�හ\\u0d98\\u0dcc'), ('හරහා', '൜\\u0bc4ො\\u0d52කෙම.')]\n",
      "  Corrected Sentence: අපි නගරයේ ෴ඊ‍දධාභ අ�වජ්අ�හ඘෌ ൜௄ො൒කෙම.\n",
      "\n",
      "Sentence 35:\n",
      "  Spelling Mistakes and Corrections: [('ව්\\u200dයාපෘතිය', 'ස\\u0dcdයාපෘති\\u0dfa'), ('සාර්ථකය.', 'කළොර\\u0dccථ\\u0d98ී\\u0d84හ.')]\n",
      "  Corrected Sentence: ඔවුන්ගේ ස෍යාපෘති෺ කළොර෌ථ඘ී඄හ.\n",
      "\n",
      "Sentence 36:\n",
      "  Spelling Mistakes and Corrections: [('කාර්යය', 'කාර්යථ'), ('ඉතා', '\\u0dc9ති'), ('සාර්ථක', '\\u0bc3වන\\u0dccද\\u0d98'), ('විය.', 'ເුලහ\\u0dfa.')]\n",
      "  Corrected Sentence: මේ කාර්යථ ෉ති ௃වන෌ද඘ ເුලහ෺.\n",
      "\n",
      "Sentence 37:\n",
      "  Spelling Mistakes and Corrections: [('නව', 'භි'), ('ව්\\u200dයාපාරයක්', 'ස\\u0dcdයාප\\u0dceරමක\\u0dcc'), ('ආරම්භ', 'අ�ළෙ\\u0d98ජ.')]\n",
      "  Corrected Sentence: ඔවුන් භි ස෍යාප෎රමක෌ අ�ළෙ඘ජ.\n",
      "\n",
      "Sentence 38:\n",
      "  Spelling Mistakes and Corrections: [('අපි', 'පි'), ('ගොවිපොළක්', 'ොව\\u0de2\\u0d98ළක්'), ('පැමිණෙමු.', '൴ඐමු.')]\n",
      "  Corrected Sentence: පි ගඟේ අසල ොව෢඘ළක් ൴ඐමු.\n",
      "\n",
      "Sentence 39:\n",
      "  Spelling Mistakes and Corrections: [('ඔහුගේ', 'ඔහුගේන'), ('ගමන', 'සාර්ථකවිය.')]\n",
      "  Corrected Sentence: ඔහුගේන සාර්ථකවිය.\n",
      "\n",
      "Sentence 40:\n",
      "  Spelling Mistakes and Corrections: [('අපි', 'අපුගිය'), ('පසුගිය', 'මාස\\u0dfaේ'), ('මාසයේ', 'ො\\u0d98කන්'), ('ගමන්', 'අ�ළෙභ�.')]\n",
      "  Corrected Sentence: අපුගිය මාස෺ේ ො඘කන් අ�ළෙභ�.\n",
      "\n",
      "Sentence 41:\n",
      "  Spelling Mistakes and Corrections: [('නායකත්වය', 'භායකතහි'), ('සාර්ථකයි.', 'සොර\\u0dccථ\\u0d98\\u0dfaී.')]\n",
      "  Corrected Sentence: ඔවුන්ගේ භායකතහි සොර෌ථ඘෺ී.\n",
      "\n",
      "Sentence 42:\n",
      "  Spelling Mistakes and Corrections: [('මම', 'ම\\u0df8'), ('කාර්යයක්', 'ේාර්ය\\u0dfaකභ�'), ('ආරම්භ', 'අ�\\u0dfbදු\\u0d98බ�'), ('කළෙමි.', '\\u09da\\u0c45ෙථි.')]\n",
      "  Corrected Sentence: ම෸ නව ේාර්ය෺කභ� අ�෻දු඘බ� ৚౅ෙථි.\n",
      "\n",
      "Sentence 43:\n",
      "  Spelling Mistakes and Corrections: [('දායකත්වය', 'දායකත\\u0dcdහථ'), ('සාර්ථකය.', 'සොරි\\u0d84\\u0dfa.')]\n",
      "  Corrected Sentence: ඔවුන්ගේ දායකත෍හථ සොරි඄෺.\n",
      "\n",
      "Sentence 44:\n",
      "  Spelling Mistakes and Corrections: [('අපි', 'පි'), ('මාර්ගයක', 'මාරහගයක'), ('ගමන්', 'ො\\u0df8න\\u0dcc'), ('කළෙම.', '൚ළෙභ�.')]\n",
      "  Corrected Sentence: පි අලුත් මාරහගයක ො෸න෌ ൚ළෙභ�.\n",
      "\n",
      "Sentence 45:\n",
      "  Spelling Mistakes and Corrections: [('වාර්තාව', 'වාර්ත\\u0dccිස'), ('නිවැරදි', 'නී\\u0dcd\\u0d98දෑ'), ('වේ.', 'ీක.')]\n",
      "  Corrected Sentence: ඔහුගේ වාර්ත෌ිස නී෍඘දෑ ీක.\n",
      "\n",
      "Sentence 46:\n",
      "  Spelling Mistakes and Corrections: [('අපේ', 'ොමනේදී'), ('ගමනේදී', 'ගැටළුකිස්ව\\u0d98\\u0dcdජහෞධෘභ�.')]\n",
      "  Corrected Sentence: ොමනේදී ගැටළුකිස්ව඘෍ජහෞධෘභ�.\n",
      "\n",
      "Sentence 47:\n",
      "  Spelling Mistakes and Corrections: [('සාර්ථක', 'සාර\\u0dccථක'), ('ව්\\u200dයාපාරයක්', '\\u0d80ඊ\\u200dයොපිජ\\u0d98ල\\u0dcd'), ('ගොඩනැගීය.', 'අ�හඩීග.')]\n",
      "  Corrected Sentence: ඔවුන් සාර෌ථක ඀ඊ‍යොපිජ඘ල෍ අ�හඩීග.\n",
      "\n",
      "Sentence 48:\n",
      "  Spelling Mistakes and Corrections: [('කාර්යය', 'කාර්යි'), ('ඉතා', '\\u0dc9ත\\u0dcc'), ('සාර්ථක', '\\u0bc3වජ\\u0d98ු'), ('වි.', 'ీී.')]\n",
      "  Corrected Sentence: මේ කාර්යි ෉ත෌ ௃වජ඘ු ీී.\n",
      "\n",
      "Sentence 49:\n",
      "  Spelling Mistakes and Corrections: [('ජීවිතය', 'ජීවිත\\u0dfa'), ('සාර්ථකයි.', 'සාර්ථකය\\u0de2.')]\n",
      "  Corrected Sentence: ඔහුගේ ජීවිත෺ සාර්ථකය෢.\n",
      "\n",
      "Sentence 50:\n",
      "  Spelling Mistakes and Corrections: [('සතුටින්', 'සතුටේන්'), ('සිටියෙම.', 'ඃවබීයෙම.')]\n",
      "  Corrected Sentence: අපි සතුටේන් ඃවබීයෙම.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "import pandas as pd\n",
    "\n",
    "# Load the fine-tuned grammar correction model and tokenizer\n",
    "model_path = \"Models/bart_grammar_checker\"\n",
    "model = BartForConditionalGeneration.from_pretrained(model_path)\n",
    "tokenizer = BartTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# Define a function to generate corrections and compare tokens\n",
    "def correct_grammar(sentence):\n",
    "    # Tokenize input and generate corrected output\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\", max_length=128, truncation=True)\n",
    "    outputs = model.generate(inputs[\"input_ids\"], max_length=128, num_beams=5, early_stopping=True)\n",
    "    corrected_sentence = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # Tokenize the original and corrected sentences\n",
    "    original_tokens = sentence.split()\n",
    "    corrected_tokens = corrected_sentence.split()\n",
    "\n",
    "    # Identify mismatches (spelling/grammar corrections)\n",
    "    mistakes = [(original, corrected) for original, corrected in zip(original_tokens, corrected_tokens) if original != corrected]\n",
    "\n",
    "    return corrected_sentence, mistakes\n",
    "\n",
    "# Load your dataset with grammar errors\n",
    "dataset_path = \"Dataset/dataset2.csv\"  # Path to your dataset\n",
    "data = pd.read_csv(dataset_path)\n",
    "\n",
    "# Generate corrections and collect results\n",
    "results = []\n",
    "for idx, row in data.iterrows():\n",
    "    grammar_error = row[\"Grammar Error\"]\n",
    "    corrected_sentence, mistakes = correct_grammar(grammar_error)\n",
    "    results.append((idx + 1, mistakes, corrected_sentence))\n",
    "\n",
    "# Print results in the required format\n",
    "for result in results:\n",
    "    sentence_id, mistakes, corrected_sentence = result\n",
    "    print(f\"Sentence {sentence_id}:\")\n",
    "    print(f\"  Spelling Mistakes and Corrections: {mistakes}\")\n",
    "    print(f\"  Corrected Sentence: {corrected_sentence}\")\n",
    "    print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
